# -*- coding: utf-8 -*-
"""Object Detection using OpenCV

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1at2gDesX1Ss6EadYCAm76bZRA-eWwj5J
"""

import cv2  #Real time computer vision and image processing tasks.
import matplotlib.pyplot as plt

config_file = '/content/ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'#The SSD(Single Shot Multibox Detector)MobileNet v3(CNN optimized for mobile and edge devices)large model is pre-trained on the COCO(Common Objects in Context)dataset, meaning it has learned to detect these 80 classes of objects.
frozen_model = '/content/frozen_inference_graph.pb' #Graph (the architecture) and weights(the learned parameters) are consolidated into a single file and makes the model ready for inference (making predictions)

model = cv2.dnn_DetectionModel(frozen_model,config_file)  #Class in OpenCV designed to load and run object detection models.

classLabels=[]
filename = '/content/labels.txt'
with open(filename,'rt') as fpt: #rt = Read text mode, fpt = file pointer, with open() construct is used because it automatically handles closing the file once the reading is done.
  classLabels = fpt.read().rstrip('\n').split('\n') #rstrip removes trailing character, Split: split into words by spaces

classLabels

len(classLabels)

model.setInputSize(320,320)           #320 x 320 pixels
model.setInputScale(1.0/127.5)        #Data is normalized to fit within a range of -1 to 1
model.setInputMean([127.5,127.5,127.5]) #By subtracting 127.5(127.5 is half of 255) from each channel, the pixel values are adjusted to center around zero(easiest for SGD).
model.setInputSwapRB(True)            #OpenCV reads images in BGR by default so swapping to RGB is used.

img = cv2.imread('boy.jpg')
plt.imshow(img)

ClassIndex, Confidence, bbox = model.detect(img,confThreshold=0.5)  #Threshold means the model will only return detections where the confidence score is 0.5 (or 50%) or higher.

print(ClassIndex)

font_scale = 3
font = cv2.FONT_HERSHEY_PLAIN
for ClassInd, Conf, bbox in zip(ClassIndex.flatten(), Confidence.flatten(),bbox): #bounding box=[x, y(x,y are coordinates), width, height], helping localize where each object is in the image
  cv2.rectangle(img, bbox, (255,0,0), 2) #Rectangle is formed with bbox, (255,0,0 is BGR), 2 is rectangle's thickeness
  cv2.putText(img, classLabels[ClassInd-1], (bbox[0]+10, bbox[1]+40), font, fontScale=font_scale, color = (0,255,0), thickness = 3) #ClassInd-1 refers to start from index 0 ,0 is X coordinate moves 10 pixels, 1 is Y

plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))

#VIDEO

cap = cv2.VideoCapture('/content/A #walk in #dublin #beautiful #weather #road #car #peace #sky #tour #video #vlog #view #clean #fresh.mp4')
if not cap.isOpened():
  cap = cv2.VideoCapture(0)
if not cap.isOpened():
  raise IOError("Cannot open video")

font_scale = 3
font = cv2.FONT_HERSHEY_PLAIN

while True:
  ret, frame = cap.read() #ret variable indicates whether the frame was successfully captured.

  ClassIndex, Confidence, bbox = model.detect(frame,confThreshold=0.55)

  print(ClassIndex)

  if (len(ClassIndex)!=0):
    for ClassInd, Conf, bbox in zip(ClassIndex.flatten(), Confidence.flatten(),bbox):
      if (ClassInd<=80): #The loop checks if the detected class index is less than or equal to 80, where class indices might correspond to certain object categories.
        cv2.rectangle(frame, bbox, (255,0,0), 2)
        cv2.putText(frame, classLabels[ClassInd-1], (bbox[0]+10, bbox[1]+40), font, fontScale=font_scale, color = (0,255,0), thickness = 3)

  cv2_imshow(frame)

  # if cv2.waitKey(2) & 0xFF == ord('q'): #This condition allows you to break the loop if the 'q' key is pressed.
  #   break
  cv2.waitkey(0)

cap.release()
cv2.destroyAllWindows()# If your application opens any windows to display images or video frames calling destroyAllWindows() ensures that all those windows are properly closed.

cap = cv2.VideoCapture(0)
if not cap.isOpened():
  cap = cv2.VideoCapture(0)
if not cap.isOpened():
  raise IOError("Cannot open video")

font_scale = 3
font = cv2.FONT_HERSHEY_PLAIN

while True:
  ret, frame = cap.read() #ret variable indicates whether the frame was successfully captured,

  ClassIndex, Confidence, bbox = model.detect(frame,confThreshold=0.55)

  print(ClassIndex)

  if (len(ClassIndex)!=0):
    for ClassInd, Conf, bbox in zip(ClassIndex.flatten(), Confidence.flatten(),bbox):
      if (ClassInd<=80): #The loop checks if the detected class index is less than or equal to 80, where class indices might correspond to certain object categories.
        cv2.rectangle(frame, bbox, (255,0,0), 2)
        cv2.putText(frame, classLabels[ClassInd-1], (bbox[0]+10, bbox[1]+40), font, fontScale=font_scale, color = (0,255,0), thickness = 3)

  cv2_imshow(frame)

  if cv2.waitKey(2) & 0xFF == ord('q'): #This condition allows you to break the loop if the 'q' key is pressed.
    break


cap.release()
cv2.destroyAllWindows()# If your application opens any windows to display images or video frames calling destroyAllWindows() ensures that all those windows are properly closed.